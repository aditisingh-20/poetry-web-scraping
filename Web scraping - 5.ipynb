{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f5bd7a",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a4f21",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0acf5d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abs</th>\n",
       "      <th>absolute</th>\n",
       "      <th>abyss</th>\n",
       "      <th>academies</th>\n",
       "      <th>accuse</th>\n",
       "      <th>accusing</th>\n",
       "      <th>actions</th>\n",
       "      <th>actual</th>\n",
       "      <th>actually</th>\n",
       "      <th>...</th>\n",
       "      <th>years</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yells</th>\n",
       "      <th>yetcalm</th>\n",
       "      <th>york</th>\n",
       "      <th>youd</th>\n",
       "      <th>youre</th>\n",
       "      <th>youve</th>\n",
       "      <th>zen</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allen</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>axentioi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christina</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iff</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judith</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuno</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paul</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammeh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 1532 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           abandon  abs  absolute  abyss  academies  accuse  accusing  \\\n",
       "allen            1    0         2      1          1       1         1   \n",
       "axentioi         0    0         0      0          0       0         0   \n",
       "christina        0    0         0      0          0       0         0   \n",
       "george           0    0         0      0          0       0         0   \n",
       "iff              0    1         0      0          0       0         0   \n",
       "judith           0    0         0      0          0       0         0   \n",
       "max              0    0         0      0          0       0         0   \n",
       "nuno             0    0         0      0          0       0         0   \n",
       "paul             0    0         0      0          0       0         0   \n",
       "sammeh           0    0         0      0          0       0         0   \n",
       "\n",
       "           actions  actual  actually  ...  years  yellow  yells  yetcalm  \\\n",
       "allen            0       1         1  ...      3       2      1        0   \n",
       "axentioi         0       0         0  ...      0       0      0        1   \n",
       "christina        0       0         0  ...      0       0      0        0   \n",
       "george           0       0         0  ...      2       0      0        0   \n",
       "iff              0       0         0  ...      0       0      0        0   \n",
       "judith           0       0         0  ...      0       0      0        0   \n",
       "max              1       0         0  ...      1       0      0        0   \n",
       "nuno             0       0         0  ...      0       0      0        0   \n",
       "paul             0       0         0  ...      0       0      0        0   \n",
       "sammeh           0       0         0  ...      0       0      0        0   \n",
       "\n",
       "           york  youd  youre  youve  zen  zoo  \n",
       "allen         1     0      3      1    1    1  \n",
       "axentioi      0     0      0      0    0    0  \n",
       "christina     0     0      0      0    0    0  \n",
       "george        0     0      0      0    0    0  \n",
       "iff           0     0      0      0    0    0  \n",
       "judith        0     0      0      0    0    0  \n",
       "max           0     0      0      0    0    0  \n",
       "nuno          0     0      0      0    0    0  \n",
       "paul          0     0      0      0    0    0  \n",
       "sammeh        0     1      0      0    0    0  \n",
       "\n",
       "[10 rows x 1532 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "73865738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ffd5b2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allen</th>\n",
       "      <th>axentioi</th>\n",
       "      <th>christina</th>\n",
       "      <th>george</th>\n",
       "      <th>iff</th>\n",
       "      <th>judith</th>\n",
       "      <th>max</th>\n",
       "      <th>nuno</th>\n",
       "      <th>paul</th>\n",
       "      <th>sammeh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolute</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abyss</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>academies</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           allen  axentioi  christina  george  iff  judith  max  nuno  paul  \\\n",
       "abandon        1         0          0       0    0       0    0     0     0   \n",
       "abs            0         0          0       0    1       0    0     0     0   \n",
       "absolute       2         0          0       0    0       0    0     0     0   \n",
       "abyss          1         0          0       0    0       0    0     0     0   \n",
       "academies      1         0          0       0    0       0    0     0     0   \n",
       "\n",
       "           sammeh  \n",
       "abandon         0  \n",
       "abs             0  \n",
       "absolute        0  \n",
       "abyss           0  \n",
       "academies       0  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6ffd09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a9de4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "25e8c4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"dog\" + 0.006*\"thee\" + 0.006*\"thy\" + 0.005*\"let\" + 0.004*\"light\" + 0.004*\"long\" + 0.004*\"shit\" + 0.004*\"doesnt\" + 0.004*\"forget\" + 0.004*\"world\"'),\n",
       " (1,\n",
       "  '0.016*\"moloch\" + 0.008*\"im\" + 0.008*\"rockland\" + 0.005*\"light\" + 0.004*\"night\" + 0.004*\"fucking\" + 0.003*\"soul\" + 0.003*\"time\" + 0.003*\"love\" + 0.003*\"streets\"')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "065e1e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"light\" + 0.004*\"charge\" + 0.004*\"owner\" + 0.004*\"purposes\" + 0.004*\"provided\" + 0.004*\"educational\" + 0.002*\"blind\" + 0.002*\"wild\" + 0.002*\"tree\" + 0.002*\"worms\"'),\n",
       " (1,\n",
       "  '0.009*\"thy\" + 0.009*\"dog\" + 0.008*\"thee\" + 0.005*\"light\" + 0.005*\"forget\" + 0.005*\"doesnt\" + 0.005*\"shit\" + 0.004*\"let\" + 0.004*\"years\" + 0.004*\"silence\"'),\n",
       " (2,\n",
       "  '0.017*\"moloch\" + 0.008*\"rockland\" + 0.008*\"im\" + 0.005*\"night\" + 0.004*\"fucking\" + 0.004*\"light\" + 0.004*\"soul\" + 0.003*\"love\" + 0.003*\"time\" + 0.003*\"denver\"')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ce4f9425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"wear\" + 0.004*\"light\" + 0.004*\"let\" + 0.004*\"educational\" + 0.004*\"owner\" + 0.004*\"world\" + 0.004*\"charge\" + 0.004*\"long\" + 0.004*\"purposes\" + 0.004*\"provided\"'),\n",
       " (1,\n",
       "  '0.014*\"fucking\" + 0.011*\"dog\" + 0.011*\"thy\" + 0.009*\"thee\" + 0.006*\"light\" + 0.006*\"shit\" + 0.006*\"doesnt\" + 0.006*\"ill\" + 0.005*\"like\" + 0.005*\"silence\"'),\n",
       " (2,\n",
       "  '0.020*\"moloch\" + 0.010*\"im\" + 0.010*\"rockland\" + 0.005*\"night\" + 0.004*\"soul\" + 0.004*\"light\" + 0.003*\"time\" + 0.003*\"denver\" + 0.003*\"streets\" + 0.003*\"jazz\"'),\n",
       " (3,\n",
       "  '0.007*\"let\" + 0.007*\"forget\" + 0.005*\"light\" + 0.005*\"hours\" + 0.005*\"world\" + 0.005*\"times\" + 0.003*\"love\" + 0.003*\"come\" + 0.003*\"eyes\" + 0.003*\"years\"')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ceb0c",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e6a1fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "63d4864e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allen</th>\n",
       "      <td>i saw the best minds of my generation destroye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>axentioi</th>\n",
       "      <td>for what you might be last withwe might not un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christina</th>\n",
       "      <td>hurt no living thing ladybird nor butterfly no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>when we two parted  in silence and tears half ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iff</th>\n",
       "      <td>loose leaves fall   from winter gray     calen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judith</th>\n",
       "      <td>the eyeless labourer in the nightthe selfless ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>let me do my work each day and if the darkened...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuno</th>\n",
       "      <td>im walking the dog its raining he sniffs and p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paul</th>\n",
       "      <td>we wear the mask that grins and liesit hides o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammeh</th>\n",
       "      <td>who the fuck did you call when you were coked ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  transcript\n",
       "allen      i saw the best minds of my generation destroye...\n",
       "axentioi   for what you might be last withwe might not un...\n",
       "christina  hurt no living thing ladybird nor butterfly no...\n",
       "george     when we two parted  in silence and tears half ...\n",
       "iff        loose leaves fall   from winter gray     calen...\n",
       "judith     the eyeless labourer in the nightthe selfless ...\n",
       "max        let me do my work each day and if the darkened...\n",
       "nuno       im walking the dog its raining he sniffs and p...\n",
       "paul       we wear the mask that grins and liesit hides o...\n",
       "sammeh     who the fuck did you call when you were coked ..."
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "884f920c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allen</th>\n",
       "      <td>i minds generation madness nakeddragging stree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>axentioi</th>\n",
       "      <td>withwe fun fun nothings anothersdo nobody this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christina</th>\n",
       "      <td>thing cricket gnat worms owner charge purposes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>silence tears years cheek colder thy truly hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iff</th>\n",
       "      <td>leaves winter gray calendar pagestwo crows lim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judith</th>\n",
       "      <td>labourer selfless shapeless seed i holdbuilds ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>work day hours despair strength desolation tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuno</th>\n",
       "      <td>im dog raining pissesbut time minutes pass dog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paul</th>\n",
       "      <td>mask liesit hides cheeks debt guilewith torn b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammeh</th>\n",
       "      <td>fuck fuck i night theyd light mom i i time lig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  transcript\n",
       "allen      i minds generation madness nakeddragging stree...\n",
       "axentioi   withwe fun fun nothings anothersdo nobody this...\n",
       "christina     thing cricket gnat worms owner charge purposes\n",
       "george     silence tears years cheek colder thy truly hou...\n",
       "iff        leaves winter gray calendar pagestwo crows lim...\n",
       "judith     labourer selfless shapeless seed i holdbuilds ...\n",
       "max        work day hours despair strength desolation tim...\n",
       "nuno       im dog raining pissesbut time minutes pass dog...\n",
       "paul       mask liesit hides cheeks debt guilewith torn b...\n",
       "sammeh     fuck fuck i night theyd light mom i i time lig..."
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2b26657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adits\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abyss</th>\n",
       "      <th>academies</th>\n",
       "      <th>actions</th>\n",
       "      <th>adonis</th>\n",
       "      <th>adorations</th>\n",
       "      <th>adult</th>\n",
       "      <th>age</th>\n",
       "      <th>airplanes</th>\n",
       "      <th>alamos</th>\n",
       "      <th>alarm</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worms</th>\n",
       "      <th>wrists</th>\n",
       "      <th>writers</th>\n",
       "      <th>yard</th>\n",
       "      <th>years</th>\n",
       "      <th>york</th>\n",
       "      <th>youve</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allen</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>axentioi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christina</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iff</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judith</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuno</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paul</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammeh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 852 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           abyss  academies  actions  adonis  adorations  adult  age  \\\n",
       "allen          1          1        0       1           1      0    0   \n",
       "axentioi       0          0        0       0           0      0    0   \n",
       "christina      0          0        0       0           0      0    0   \n",
       "george         0          0        0       0           0      0    0   \n",
       "iff            0          0        0       0           0      0    0   \n",
       "judith         0          0        0       0           0      0    0   \n",
       "max            0          0        1       0           0      0    1   \n",
       "nuno           0          0        0       0           0      0    0   \n",
       "paul           0          0        0       0           0      0    0   \n",
       "sammeh         0          0        0       0           0      1    0   \n",
       "\n",
       "           airplanes  alamos  alarm  ...  work  world  worms  wrists  writers  \\\n",
       "allen              1       1      1  ...     0      1      1       1        1   \n",
       "axentioi           0       0      0  ...     0      0      0       0        0   \n",
       "christina          0       0      0  ...     0      0      1       0        0   \n",
       "george             0       0      0  ...     0      0      0       0        0   \n",
       "iff                0       0      0  ...     0      0      0       0        0   \n",
       "judith             0       0      0  ...     0      0      0       0        0   \n",
       "max                0       0      0  ...     1      2      0       0        0   \n",
       "nuno               0       0      0  ...     0      0      0       0        0   \n",
       "paul               0       0      0  ...     0      2      0       0        0   \n",
       "sammeh             0       0      0  ...     0      0      0       0        0   \n",
       "\n",
       "           yard  years  york  youve  zoo  \n",
       "allen         1      3     1      1    1  \n",
       "axentioi      0      0     0      0    0  \n",
       "christina     0      0     0      0    0  \n",
       "george        0      2     0      0    0  \n",
       "iff           0      0     0      0    0  \n",
       "judith        0      0     0      0    0  \n",
       "max           0      1     0      0    0  \n",
       "nuno          0      0     0      0    0  \n",
       "paul          0      0     0      0    0  \n",
       "sammeh        0      0     0      0    0  \n",
       "\n",
       "[10 rows x 852 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "79e2645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "44691b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"moloch\" + 0.014*\"rockland\" + 0.007*\"night\" + 0.007*\"light\" + 0.006*\"soul\" + 0.005*\"dreams\" + 0.005*\"river\" + 0.005*\"eyes\" + 0.005*\"streets\" + 0.004*\"life\"'),\n",
       " (1,\n",
       "  '0.012*\"dog\" + 0.009*\"thee\" + 0.007*\"owner\" + 0.007*\"purposes\" + 0.007*\"charge\" + 0.007*\"doesnt\" + 0.006*\"tears\" + 0.006*\"silence\" + 0.004*\"light\" + 0.004*\"years\"')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "55087b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"dog\" + 0.012*\"thee\" + 0.010*\"doesnt\" + 0.008*\"light\" + 0.008*\"owner\" + 0.008*\"tears\" + 0.008*\"purposes\" + 0.008*\"charge\" + 0.008*\"silence\" + 0.005*\"life\"'),\n",
       " (1,\n",
       "  '0.008*\"light\" + 0.006*\"charge\" + 0.006*\"owner\" + 0.006*\"purposes\" + 0.006*\"world\" + 0.006*\"life\" + 0.006*\"hours\" + 0.006*\"times\" + 0.006*\"strength\" + 0.006*\"childs\"'),\n",
       " (2,\n",
       "  '0.021*\"moloch\" + 0.016*\"rockland\" + 0.008*\"night\" + 0.007*\"soul\" + 0.005*\"streets\" + 0.005*\"light\" + 0.005*\"dreams\" + 0.005*\"eyes\" + 0.005*\"mind\" + 0.005*\"jazz\"')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d476e56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"dog\" + 0.014*\"thee\" + 0.013*\"doesnt\" + 0.010*\"silence\" + 0.007*\"tears\" + 0.007*\"years\" + 0.007*\"hour\" + 0.007*\"shiti\" + 0.004*\"light\" + 0.004*\"morning\"'),\n",
       " (1,\n",
       "  '0.022*\"moloch\" + 0.017*\"rockland\" + 0.008*\"night\" + 0.007*\"soul\" + 0.006*\"streets\" + 0.005*\"dreams\" + 0.005*\"light\" + 0.005*\"jazz\" + 0.005*\"eyes\" + 0.005*\"mind\"'),\n",
       " (2,\n",
       "  '0.008*\"fun\" + 0.008*\"charge\" + 0.008*\"childs\" + 0.008*\"purposes\" + 0.008*\"owner\" + 0.005*\"light\" + 0.005*\"love\" + 0.005*\"worms\" + 0.005*\"poetry\" + 0.005*\"poetrybut\"'),\n",
       " (3,\n",
       "  '0.013*\"light\" + 0.013*\"world\" + 0.010*\"life\" + 0.007*\"owner\" + 0.007*\"purposes\" + 0.007*\"charge\" + 0.007*\"hours\" + 0.007*\"mother\" + 0.007*\"times\" + 0.007*\"fuck\"')]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f27c9",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "05a14186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "af352ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allen</th>\n",
       "      <td>i best minds generation madness hysterical nak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>axentioi</th>\n",
       "      <td>last withwe fun fun nothings anothersdo advert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christina</th>\n",
       "      <td>thing ladybird butterfly moth dusty cricket gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>silence tears years pale thy cheek cold colder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iff</th>\n",
       "      <td>loose leaves winter gray calendar pagestwo cro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judith</th>\n",
       "      <td>eyeless labourer nightthe selfless shapeless s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>work day darkened hours despair strength desol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuno</th>\n",
       "      <td>im dog raining pissesbut time minutes pass dog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paul</th>\n",
       "      <td>mask liesit hides cheeks eyesthis debt human g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammeh</th>\n",
       "      <td>fuck fuck fuck i night theyd more light mom i ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  transcript\n",
       "allen      i best minds generation madness hysterical nak...\n",
       "axentioi   last withwe fun fun nothings anothersdo advert...\n",
       "christina  thing ladybird butterfly moth dusty cricket gr...\n",
       "george     silence tears years pale thy cheek cold colder...\n",
       "iff        loose leaves winter gray calendar pagestwo cro...\n",
       "judith     eyeless labourer nightthe selfless shapeless s...\n",
       "max        work day darkened hours despair strength desol...\n",
       "nuno       im dog raining pissesbut time minutes pass dog...\n",
       "paul       mask liesit hides cheeks eyesthis debt human g...\n",
       "sammeh     fuck fuck fuck i night theyd more light mom i ..."
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4d0047d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adits\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs</th>\n",
       "      <th>absolute</th>\n",
       "      <th>abyss</th>\n",
       "      <th>academies</th>\n",
       "      <th>actions</th>\n",
       "      <th>actual</th>\n",
       "      <th>adonis</th>\n",
       "      <th>adorations</th>\n",
       "      <th>adult</th>\n",
       "      <th>advertismentsnot</th>\n",
       "      <th>...</th>\n",
       "      <th>worms</th>\n",
       "      <th>wrists</th>\n",
       "      <th>writers</th>\n",
       "      <th>yard</th>\n",
       "      <th>years</th>\n",
       "      <th>yellow</th>\n",
       "      <th>york</th>\n",
       "      <th>youve</th>\n",
       "      <th>zen</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allen</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>axentioi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christina</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iff</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judith</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuno</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paul</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammeh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 1104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           abs  absolute  abyss  academies  actions  actual  adonis  \\\n",
       "allen        0         2      1          1        0       1       1   \n",
       "axentioi     0         0      0          0        0       0       0   \n",
       "christina    0         0      0          0        0       0       0   \n",
       "george       0         0      0          0        0       0       0   \n",
       "iff          1         0      0          0        0       0       0   \n",
       "judith       0         0      0          0        0       0       0   \n",
       "max          0         0      0          0        1       0       0   \n",
       "nuno         0         0      0          0        0       0       0   \n",
       "paul         0         0      0          0        0       0       0   \n",
       "sammeh       0         0      0          0        0       0       0   \n",
       "\n",
       "           adorations  adult  advertismentsnot  ...  worms  wrists  writers  \\\n",
       "allen               1      0                 0  ...      1       1        1   \n",
       "axentioi            0      0                 1  ...      0       0        0   \n",
       "christina           0      0                 0  ...      1       0        0   \n",
       "george              0      0                 0  ...      0       0        0   \n",
       "iff                 0      0                 0  ...      0       0        0   \n",
       "judith              0      0                 0  ...      0       0        0   \n",
       "max                 0      0                 0  ...      0       0        0   \n",
       "nuno                0      0                 0  ...      0       0        0   \n",
       "paul                0      0                 0  ...      0       0        0   \n",
       "sammeh              0      1                 0  ...      0       0        0   \n",
       "\n",
       "           yard  years  yellow  york  youve  zen  zoo  \n",
       "allen         1      3       2     1      1    1    1  \n",
       "axentioi      0      0       0     0      0    0    0  \n",
       "christina     0      0       0     0      0    0    0  \n",
       "george        0      2       0     0      0    0    0  \n",
       "iff           0      0       0     0      0    0    0  \n",
       "judith        0      0       0     0      0    0    0  \n",
       "max           0      1       0     0      0    0    0  \n",
       "nuno          0      0       0     0      0    0    0  \n",
       "paul          0      0       0     0      0    0    0  \n",
       "sammeh        0      0       0     0      0    0    0  \n",
       "\n",
       "[10 rows x 1104 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "590610fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "31b9e6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"world\" + 0.005*\"charge\" + 0.005*\"owner\" + 0.005*\"educational\" + 0.005*\"purposes\" + 0.005*\"light\" + 0.003*\"hours\" + 0.003*\"life\" + 0.003*\"times\" + 0.003*\"mask\"'),\n",
       " (1,\n",
       "  '0.013*\"moloch\" + 0.010*\"rockland\" + 0.006*\"light\" + 0.006*\"night\" + 0.004*\"soul\" + 0.004*\"thy\" + 0.004*\"dog\" + 0.003*\"love\" + 0.003*\"streets\" + 0.003*\"thee\"')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "019fbadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"moloch\" + 0.012*\"rockland\" + 0.006*\"night\" + 0.005*\"soul\" + 0.005*\"dog\" + 0.005*\"light\" + 0.004*\"streets\" + 0.003*\"river\" + 0.003*\"endless\" + 0.003*\"eyes\"'),\n",
       " (1,\n",
       "  '0.014*\"thy\" + 0.012*\"thee\" + 0.007*\"owner\" + 0.007*\"purposes\" + 0.007*\"educational\" + 0.007*\"charge\" + 0.007*\"silence\" + 0.007*\"tears\" + 0.005*\"light\" + 0.005*\"years\"'),\n",
       " (2,\n",
       "  '0.010*\"light\" + 0.008*\"fuck\" + 0.006*\"life\" + 0.006*\"best\" + 0.006*\"fucking\" + 0.004*\"love\" + 0.004*\"charge\" + 0.004*\"educational\" + 0.004*\"purposes\" + 0.004*\"owner\"')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5a4fbedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"thy\" + 0.014*\"thee\" + 0.010*\"educational\" + 0.010*\"charge\" + 0.010*\"purposes\" + 0.010*\"owner\" + 0.008*\"light\" + 0.008*\"tears\" + 0.008*\"silence\" + 0.005*\"world\"'),\n",
       " (1,\n",
       "  '0.020*\"dog\" + 0.012*\"doesnt\" + 0.009*\"fuck\" + 0.009*\"best\" + 0.009*\"fucking\" + 0.006*\"light\" + 0.006*\"pst\" + 0.006*\"life\" + 0.006*\"mother\" + 0.006*\"ill\"'),\n",
       " (2,\n",
       "  '0.006*\"light\" + 0.006*\"life\" + 0.006*\"spare\" + 0.006*\"hours\" + 0.006*\"times\" + 0.006*\"world\" + 0.003*\"eyes\" + 0.003*\"pst\" + 0.003*\"winter\" + 0.003*\"death\"'),\n",
       " (3,\n",
       "  '0.017*\"moloch\" + 0.013*\"rockland\" + 0.006*\"night\" + 0.006*\"soul\" + 0.005*\"light\" + 0.004*\"streets\" + 0.004*\"love\" + 0.004*\"endless\" + 0.004*\"mind\" + 0.004*\"eyes\"')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae6c9d",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1d099166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"thy\" + 0.016*\"thee\" + 0.009*\"owner\" + 0.009*\"purposes\" + 0.009*\"educational\" + 0.009*\"charge\" + 0.009*\"silence\" + 0.009*\"tears\" + 0.006*\"years\" + 0.006*\"mask\"'),\n",
       " (1,\n",
       "  '0.001*\"pst\" + 0.001*\"spare\" + 0.001*\"incrementsa\" + 0.001*\"staccato\" + 0.001*\"family\" + 0.001*\"photo\" + 0.001*\"sputum\" + 0.001*\"leaves\" + 0.001*\"jan\" + 0.001*\"fragile\"'),\n",
       " (2,\n",
       "  '0.017*\"moloch\" + 0.013*\"rockland\" + 0.006*\"night\" + 0.006*\"soul\" + 0.005*\"light\" + 0.004*\"streets\" + 0.004*\"river\" + 0.004*\"dreams\" + 0.004*\"eyes\" + 0.004*\"jazz\"'),\n",
       " (3,\n",
       "  '0.014*\"dog\" + 0.010*\"light\" + 0.008*\"doesnt\" + 0.008*\"fuck\" + 0.006*\"pst\" + 0.006*\"life\" + 0.006*\"best\" + 0.006*\"fucking\" + 0.004*\"world\" + 0.004*\"times\"')]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final LDA model\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18382ec9",
   "metadata": {},
   "source": [
    "* Topic 0: old words(thy/thee)\n",
    "* Topic 1: family\n",
    "* Topic 2: night, dreams\n",
    "* Topic 3: profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "72255bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(2, 0.99934334)], 'allen'),\n",
       " ([(3, 0.97109103)], 'axentioi'),\n",
       " ([(0, 0.96224535), (1, 0.012516063), (2, 0.0125650745), (3, 0.012673496)],\n",
       "  'christina'),\n",
       " ([(0, 0.98629403)], 'george'),\n",
       " ([(2, 0.9769132)], 'iff'),\n",
       " ([(3, 0.98427)], 'judith'),\n",
       " ([(3, 0.988686)], 'max'),\n",
       " ([(3, 0.98433846)], 'nuno'),\n",
       " ([(0, 0.97780305)], 'paul'),\n",
       " ([(3, 0.9824626)], 'sammeh')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for a in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee88f52",
   "metadata": {},
   "source": [
    "* Topic 0: old words(thy/thee) [Christina, George, Paul]\n",
    "* Topic 1: family [Christina]\n",
    "* Topic 2: night, dreams [Allen, Iff, Christina]\n",
    "* Topic 3: profanity [Judith, Max, Nuno, Sammeh]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7472e",
   "metadata": {},
   "source": [
    "## Additional Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8745f0fd",
   "metadata": {},
   "source": [
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "822e950f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"moloch\" + 0.013*\"rockland\" + 0.006*\"night\" + 0.006*\"light\" + 0.005*\"soul\" + 0.004*\"streets\" + 0.003*\"eyes\" + 0.003*\"dreams\" + 0.003*\"river\" + 0.003*\"mind\"'),\n",
       " (1,\n",
       "  '0.018*\"dog\" + 0.010*\"doesnt\" + 0.005*\"world\" + 0.005*\"light\" + 0.005*\"times\" + 0.005*\"good\" + 0.005*\"shit\" + 0.005*\"shiti\" + 0.005*\"fun\" + 0.005*\"hours\"'),\n",
       " (2,\n",
       "  '0.001*\"dancing\" + 0.001*\"moth\" + 0.001*\"harmless\" + 0.001*\"thing\" + 0.001*\"fat\" + 0.001*\"gnat\" + 0.001*\"cricket\" + 0.001*\"ladybird\" + 0.001*\"butterfly\" + 0.001*\"dusty\"'),\n",
       " (3,\n",
       "  '0.018*\"thy\" + 0.013*\"thee\" + 0.008*\"fuck\" + 0.008*\"light\" + 0.008*\"life\" + 0.008*\"best\" + 0.008*\"silence\" + 0.008*\"fucking\" + 0.006*\"tears\" + 0.006*\"years\"')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=1000)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53950bd4",
   "metadata": {},
   "source": [
    "* Topic 0: night, dreams\n",
    "* Topic 1: profanity\n",
    "* Topic 2: moth, gnat(insects)\n",
    "* Topic 3: old words(thy/thee)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd34dec",
   "metadata": {},
   "source": [
    "Here, I have added a new topic 'insects' which fits better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d3d2474a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0, 0.99934417)], 'allen'),\n",
       " ([(1, 0.970945)], 'axentioi'),\n",
       " ([(0, 0.9620121), (1, 0.012701472), (2, 0.012560154), (3, 0.01272626)],\n",
       "  'christina'),\n",
       " ([(3, 0.986257)], 'george'),\n",
       " ([(3, 0.9771922)], 'iff'),\n",
       " ([(0, 0.98419696)], 'judith'),\n",
       " ([(1, 0.98866916)], 'max'),\n",
       " ([(1, 0.9843395)], 'nuno'),\n",
       " ([(0, 0.9772444)], 'paul'),\n",
       " ([(3, 0.982432)], 'sammeh')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for a in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e220c7da",
   "metadata": {},
   "source": [
    "* Topic 0: night, dreams [Allen, Christina, Judith, Paul]\n",
    "* Topic 1: profanity [Axentioi, Max, Nuno, Christina]\n",
    "* Topic 2: moth, gnat(insects) [Christina]\n",
    "* Topic 3: old words(thy/thee) [George, Iff, Sammeh, Christina]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa1176",
   "metadata": {},
   "source": [
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1a5cc",
   "metadata": {},
   "source": [
    "Here, I have made a new topic model that includes nouns, adjectives and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "56057404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_adj_verbs(text):\n",
    "    is_nouns_adj_verbs = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ' or pos[:2] == 'VB'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj_verbs = [word for (word, pos) in pos_tag(tokenized) if is_nouns_adj_verbs(pos)] \n",
    "    return ' '.join(nouns_adj_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2e34c2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allen</th>\n",
       "      <td>i saw best minds generation destroyed madness ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>axentioi</th>\n",
       "      <td>be last withwe understandits fun gets fun noth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christina</th>\n",
       "      <td>hurt living thing ladybird butterfly moth dust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>parted silence tears brokenhearted sever years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iff</th>\n",
       "      <td>loose leaves fall winter gray calendar pagestw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judith</th>\n",
       "      <td>eyeless labourer nightthe selfless shapeless s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>let do work day darkened hours despair i forge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuno</th>\n",
       "      <td>im walking dog raining sniffs pissesbut takes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paul</th>\n",
       "      <td>wear mask grins liesit hides cheeks shades eye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammeh</th>\n",
       "      <td>fuck did call were coked fuck started cry thou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  transcript\n",
       "allen      i saw best minds generation destroyed madness ...\n",
       "axentioi   be last withwe understandits fun gets fun noth...\n",
       "christina  hurt living thing ladybird butterfly moth dust...\n",
       "george     parted silence tears brokenhearted sever years...\n",
       "iff        loose leaves fall winter gray calendar pagestw...\n",
       "judith     eyeless labourer nightthe selfless shapeless s...\n",
       "max        let do work day darkened hours despair i forge...\n",
       "nuno       im walking dog raining sniffs pissesbut takes ...\n",
       "paul       wear mask grins liesit hides cheeks shades eye...\n",
       "sammeh     fuck did call were coked fuck started cry thou..."
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns_adj_verbs = pd.DataFrame(data_clean.transcript.apply(nouns_adj_verbs))\n",
    "data_nouns_adj_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d014ce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adits\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abs</th>\n",
       "      <th>absolute</th>\n",
       "      <th>abyss</th>\n",
       "      <th>academies</th>\n",
       "      <th>accuse</th>\n",
       "      <th>accusing</th>\n",
       "      <th>actions</th>\n",
       "      <th>actual</th>\n",
       "      <th>addicted</th>\n",
       "      <th>...</th>\n",
       "      <th>yacketayakking</th>\n",
       "      <th>yard</th>\n",
       "      <th>years</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yells</th>\n",
       "      <th>york</th>\n",
       "      <th>youd</th>\n",
       "      <th>youve</th>\n",
       "      <th>zen</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allen</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>axentioi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christina</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>george</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iff</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judith</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuno</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paul</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sammeh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 1474 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           abandon  abs  absolute  abyss  academies  accuse  accusing  \\\n",
       "allen            1    0         2      1          1       1         1   \n",
       "axentioi         0    0         0      0          0       0         0   \n",
       "christina        0    0         0      0          0       0         0   \n",
       "george           0    0         0      0          0       0         0   \n",
       "iff              0    1         0      0          0       0         0   \n",
       "judith           0    0         0      0          0       0         0   \n",
       "max              0    0         0      0          0       0         0   \n",
       "nuno             0    0         0      0          0       0         0   \n",
       "paul             0    0         0      0          0       0         0   \n",
       "sammeh           0    0         0      0          0       0         0   \n",
       "\n",
       "           actions  actual  addicted  ...  yacketayakking  yard  years  \\\n",
       "allen            0       1         0  ...               1     1      3   \n",
       "axentioi         0       0         0  ...               0     0      0   \n",
       "christina        0       0         0  ...               0     0      0   \n",
       "george           0       0         0  ...               0     0      2   \n",
       "iff              0       0         0  ...               0     0      0   \n",
       "judith           0       0         0  ...               0     0      0   \n",
       "max              1       0         0  ...               0     0      1   \n",
       "nuno             0       0         0  ...               0     0      0   \n",
       "paul             0       0         0  ...               0     0      0   \n",
       "sammeh           0       0         1  ...               0     0      0   \n",
       "\n",
       "           yellow  yells  york  youd  youve  zen  zoo  \n",
       "allen           2      1     1     0      1    1    1  \n",
       "axentioi        0      0     0     0      0    0    0  \n",
       "christina       0      0     0     0      0    0    0  \n",
       "george          0      0     0     0      0    0    0  \n",
       "iff             0      0     0     0      0    0    0  \n",
       "judith          0      0     0     0      0    0    0  \n",
       "max             0      0     0     0      0    0    0  \n",
       "nuno            0      0     0     0      0    0    0  \n",
       "paul            0      0     0     0      0    0    0  \n",
       "sammeh          0      0     0     1      0    0    0  \n",
       "\n",
       "[10 rows x 1474 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvnav = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvnav = cvnav.fit_transform(data_nouns_adj_verbs.transcript)\n",
    "data_dtmnav = pd.DataFrame(data_cvnav.toarray(), columns=cvnav.get_feature_names())\n",
    "data_dtmnav.index = data_nouns_adj_verbs.index\n",
    "data_dtmnav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d30ebdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusnav = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmnav.transpose()))\n",
    "\n",
    "id2wordnav = dict((v, k) for k, v in cvnav.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fed9eaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"light\" + 0.003*\"childs\" + 0.003*\"eyesthis\" + 0.003*\"pst\" + 0.003*\"educational\" + 0.003*\"owner\" + 0.003*\"provided\" + 0.003*\"purposes\" + 0.003*\"charge\" + 0.003*\"hold\"'),\n",
       " (1,\n",
       "  '0.018*\"moloch\" + 0.009*\"rockland\" + 0.004*\"night\" + 0.004*\"light\" + 0.004*\"soul\" + 0.003*\"thy\" + 0.003*\"thee\" + 0.003*\"streets\" + 0.003*\"river\" + 0.003*\"dreams\"'),\n",
       " (2,\n",
       "  '0.018*\"fucking\" + 0.014*\"dog\" + 0.008*\"ill\" + 0.008*\"shit\" + 0.008*\"doesnt\" + 0.006*\"fuck\" + 0.006*\"best\" + 0.004*\"pst\" + 0.004*\"life\" + 0.004*\"love\"'),\n",
       " (3,\n",
       "  '0.007*\"let\" + 0.007*\"forget\" + 0.005*\"world\" + 0.005*\"light\" + 0.005*\"times\" + 0.005*\"hours\" + 0.003*\"owner\" + 0.003*\"provided\" + 0.003*\"educational\" + 0.003*\"charge\"')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldanav = models.LdaModel(corpus=corpusnav, num_topics=4, id2word=id2wordnav, passes=1000)\n",
    "ldanav.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4a2d4",
   "metadata": {},
   "source": [
    "* Topic 0: child, educational\n",
    "* Topic 1: night, dreams\n",
    "* Topic 2: profanity\n",
    "* Topic 3: forgetting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80dc14",
   "metadata": {},
   "source": [
    "Here, I have added a new topic 'forgetting' which is a verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "442dcfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'allen'),\n",
       " (1, 'axentioi'),\n",
       " (1, 'christina'),\n",
       " (1, 'george'),\n",
       " (0, 'iff'),\n",
       " (0, 'judith'),\n",
       " (3, 'max'),\n",
       " (2, 'nuno'),\n",
       " (1, 'paul'),\n",
       " (2, 'sammeh')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_transformed = ldanav[corpusnav]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmnav.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37425a76",
   "metadata": {},
   "source": [
    "* Topic 0: child, educational [Iff, Judith]\n",
    "* Topic 1: night, dreams [Allen, Axentioi, Christina, George, Paul]\n",
    "* Topic 2: profanity [Nuno, Sammeh]\n",
    "* Topic 3: forgetting [Max]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
